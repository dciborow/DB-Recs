{
    "cells": [{
        "cell_type": "markdown",
        "source": ["### ALS Movie Example Databricks Notebook\n##### by Daniel Ciborowski, dciborow@microsoft.com\n\n##### Copyright (c) Microsoft Corporation. All rights reserved.\n\n##### Licensed under the MIT License.\n\n##### Setup\n1. Create new Cluster, DB 4.1, Spark 2.3.0, Python3\n1. Add Azure-cli via pypi - azure-cli\n1. Add AzureML via Pypi - azureml-sdk[databricks]\n1. Add pydocumentdb via Pypi\n1. Add CosmosDB uber jar via maven central - https://search.maven.org/artifact/com.microsoft.azure/azure-cosmosdb-spark_2.3.0_2.11/1.2.2\n\n[See here for help adding a library.](https://docs.databricks.com/user-guide/libraries.html)\n\n##### This notebook is broken down into four sections.\n1. Service Creation\n1. Training\n1. Scoring\n1. Operationalization\n\n##### The following Azure services will be deployed into a new or existing resource group.\n1. [ML Service](https://docs.databricks.com/user-guide/libraries.html)\n1. [Cosmos DB](https://azure.microsoft.com/en-us/services/cosmos-db/)\n1. [Container Registery](https://docs.microsoft.com/en-us/azure/container-registry/)\n1. [Container Instances](https://docs.microsoft.com/en-us/azure/container-instances/)\n1. [Application Insights](https://azure.microsoft.com/en-us/services/monitor/)\n1. Storage Account\n1. Key Vault"],
        "metadata": {}
    }, {
        "cell_type": "markdown",
        "source": ["# I. Service Creation"],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["from azure.common.client_factory import get_client_from_cli_profile\nfrom azure.mgmt.compute import ComputeManagementClient\nimport azure.mgmt.cosmosdb\n\nimport azureml.core\nfrom azureml.core import Workspace\nfrom azureml.core.run import Run\nfrom azureml.core.experiment import Experiment\n\nimport pydocumentdb\nimport pydocumentdb.document_client as document_client\n\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.sql import Row\n\nimport numpy as np\nimport os\nimport pandas as pd\nimport pprint\nimport shutil\nimport time, timeit\nimport urllib\nimport yaml\n\n# Check core SDK version number - based on build number of preview/master.\nprint(\"SDK version:\", azureml.core.VERSION)"],
        "metadata": {},
        "outputs": [],
        "execution_count": 3
    }, {
        "cell_type": "code",
        "source": ["prefix = dbutils.widgets.get(\"Prefix\")\ndata = dbutils.widgets.get(\"Dataset\")\nalgo = dbutils.widgets.get(\"Algo\")\n\nresource_group = prefix + \"_\" + data\nworkspace_name = prefix + \"_\"+data+\"_aml\"\nworkspace_region = \"westus2\"\n\n#Columns\nuserCol=dbutils.widgets.get(\"Column User\")\nitemCol=dbutils.widgets.get(\"Column Item\")\nratingCol=dbutils.widgets.get(\"Column Rating\")\n\nuserColIndex = userCol.replace(\"Id\",\"Index\")\nitemColIndex = itemCol.replace(\"Id\",\"Index\")\n\n#CosmosDB\nlocation = 'westus2'\naccount_name = prefix + \"-\" + data + \"-ds-sql\"\nDOCUMENTDB_DATABASE = \"recommendations\"\nDOCUMENTDB_COLLECTION = \"user_recommendations_\" + algo\n\n#AzureML\nhistory_name = 'spark-ml-notebook'\nmodel_name = data+\"-\"+algo+\"-Recommendations.mml\"\nservice_name = data + \"-\" + algo\nexperiment_name = data + \"_\"+ algo +\"_Experiment\"\n\ntrain_data_path = data + \"Train\"\ntest_data_path = data + \"Test\"\n\n#AKS\naks_name = prefix + '-' + algo + '-aks' \n#unique service name\nservice_name = prefix + '-' + algo + '-service'\n\n\nsubscription_id = dbutils.widgets.get(\"Subscription\")"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]
            }
        }],
        "execution_count": 4
    }, {
        "cell_type": "code",
        "source": ["# import the Workspace class and check the azureml SDK version\n# exist_ok checks if workspace exists or not.\nws = Workspace.create(name = workspace_name,\n                      subscription_id = subscription_id,\n                      resource_group = resource_group, \n                      location = workspace_region,\n                      exist_ok=True)\n\n# persist the subscription id, resource group name, and workspace name in aml_config/config.json.\nws.write_config()"],
        "metadata": {},
        "outputs": [],
        "execution_count": 5
    }, {
        "cell_type": "code",
        "source": ["def find_collection(client, dbid, id):\n        database_link = 'dbs/' + dbid\n        collections = list(client.QueryCollections(\n            database_link,\n            {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    { \"name\":\"@id\", \"value\": id }\n                ]\n            }\n        ))\n\n        if len(collections) > 0:\n            return True\n        else:\n            return False\ndef read_collection(client, dbid, id):\n        try:\n            database_link = 'dbs/' + dbid\n            collection_link = database_link + '/colls/{0}'.format(id)\n\n            collection = client.ReadCollection(collection_link)\n            return collection\n        except errors.DocumentDBError as e:\n            if e.status_code == 404:\n               print('A collection with id \\'{0}\\' does not exist'.format(id))\n            else: \n                raise errors.HTTPFailure(e.status_code)    \n\ndef read_database(client, id):\n    try:\n        database_link = 'dbs/' + id\n\n        database = client.ReadDatabase(database_link)\n        return database\n    except errors.DocumentDBError as e:\n        if e.status_code == 404:\n           print('A database with id \\'{0}\\' does not exist'.format(id))\n        else: \n            raise errors.HTTPFailure(e.status_code)  \n            \ndef find_database(client, id):\n        databases = list(client.QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                { \"name\":\"@id\", \"value\": id }\n            ]\n        }))\n\n        if len(databases) > 0:\n            return True\n        else:\n            return False            \n\nclient = get_client_from_cli_profile(azure.mgmt.cosmosdb.CosmosDB)\n\nasync_cosmosdb_create = client.database_accounts.create_or_update(\n    resource_group,\n    account_name,\n    {\n        'location': location,\n        'locations': [{\n            'location_name': location\n        }]\n    }\n)\naccount = async_cosmosdb_create.result()\n\nmy_keys = client.database_accounts.list_keys(\n    resource_group,\n    account_name\n)\nmaster_key = my_keys.primary_master_key\nendpoint = \"https://\" + account_name + \".documents.azure.com:443/\"\n#db client\nclient = document_client.DocumentClient(endpoint, {'masterKey': master_key})\n\nif find_database(client, DOCUMENTDB_DATABASE) == False:\n  db = client.CreateDatabase({ 'id': DOCUMENTDB_DATABASE })\nelse:\n  db = read_database(client, DOCUMENTDB_DATABASE)\n# Create collection options\noptions = {\n    'offerThroughput': 11000\n}\n\n# Create a collection\ncollection_definition = { 'id': DOCUMENTDB_COLLECTION, 'partitionKey': {'paths': ['/id'],'kind': 'Hash'} }\nif find_collection(client,DOCUMENTDB_DATABASE,  DOCUMENTDB_COLLECTION) ==False:\n  collection = client.CreateCollection(db['_self'], collection_definition, options)\nelse:\n  collection = read_collection(client, DOCUMENTDB_DATABASE, DOCUMENTDB_COLLECTION)\nsecrets = {\n  \"Endpoint\": endpoint,\n  \"Masterkey\": master_key,\n  \"Database\": DOCUMENTDB_DATABASE,\n  \"Collection\": DOCUMENTDB_COLLECTION,\n  \"Upsert\": \"true\"\n}\nimport json\nwith open(\"secrets.json\", \"w\") as file:\n    json.dump(secrets, file)"],
        "metadata": {},
        "outputs": [],
        "execution_count": 6
    }, {
        "cell_type": "markdown",
        "source": ["# II. Training"],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["# Download Movie Lens\nbasedataurl = \"http://aka.ms\" \ndatafile = \"MovieRatings.csv\"\n\ndatafile_dbfs = os.path.join(\"/dbfs\", datafile)\n\nif os.path.isfile(datafile_dbfs):\n    print(\"found {} at {}\".format(datafile, datafile_dbfs))\nelse:\n    print(\"downloading {} to {}\".format(datafile, datafile_dbfs))\n    urllib.request.urlretrieve(os.path.join(basedataurl, datafile), datafile_dbfs)\n    \ndata_all = sqlContext.read.format('csv')\\\n                     .options(header='true', delimiter=',', inferSchema='true', ignoreLeadingWhiteSpace='true', ignoreTrailingWhiteSpace='true')\\\n                     .load(datafile)    \ndata_all.printSchema()\ndisplay(data_all)"],
        "metadata": {},
        "outputs": [],
        "execution_count": 8
    }, {
        "cell_type": "code",
        "source": ["train, test = data_all.cache().randomSplit([0.75, 0.25], seed=123)\n\nprint(\"train ({}, {})\".format(train.cache().count(), len(train.columns)))\nprint(\"test ({}, {})\".format(test.cache().count(), len(test.columns)))\n\ntrain_data_path_dbfs = os.path.join(\"/dbfs\", train_data_path)\ntest_data_path_dbfs = os.path.join(\"/dbfs\", test_data_path)\n\ntrain.write.mode('overwrite').parquet(train_data_path)\ntest.write.mode('overwrite').parquet(test_data_path)\nprint(\"train and test datasets saved to {} and {}\".format(train_data_path_dbfs, test_data_path_dbfs))"],
        "metadata": {},
        "outputs": [],
        "execution_count": 9
    }, {
        "cell_type": "code",
        "source": ["#from tqdm import tqdm\nmodel_dbfs = os.path.join(\"/dbfs\", model_name)\n\n# start a training run by defining an experiment\nmyexperiment = Experiment(ws, experiment_name)\nroot_run = myexperiment.start_logging()"],
        "metadata": {},
        "outputs": [],
        "execution_count": 10
    }, {
        "cell_type": "code",
        "source": ["indexerContacts = StringIndexer(inputCol=userCol, outputCol=userColIndex, handleInvalid='keep').fit(data_all)\nindexerRules = StringIndexer(inputCol=itemCol, outputCol=itemColIndex, handleInvalid='keep').fit(data_all)\n\nals = ALS(maxIter=5, userCol=userColIndex, itemCol=itemColIndex, ratingCol=ratingCol, coldStartStrategy=\"drop\")\n\n# put together the pipeline\npipe = Pipeline(stages=[indexerContacts, indexerRules, als])"],
        "metadata": {},
        "outputs": [],
        "execution_count": 11
    }, {
        "cell_type": "code",
        "source": ["# Regularization Rates\nregs = [1, 0.1, 0.01, 0.001]\nparamGrid = ParamGridBuilder().addGrid(als.regParam, regs).build()\n\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=ratingCol, predictionCol=\"prediction\")\ncv = CrossValidator(estimator=pipe, evaluator=evaluator, estimatorParamMaps=paramGrid)\ntrain.cache()\ncvModel = cv.fit(train)\ni = 0"],
        "metadata": {},
        "outputs": [],
        "execution_count": 12
    }, {
        "cell_type": "code",
        "source": ["# record a bunch of reg values in a ALS model\nfor reg in regs:\n    print(reg)\n    # create a bunch of child runs\n    with root_run.child_run(\"reg-\" + str(reg)) as run:\n        rmse = cvModel.avgMetrics[i]\n        print(\"Root-mean-square error = \" + str(rmse))\n        \n        # log reg, rmse and feature names in run history\n        run.log(\"reg\", reg)\n        run.log(\"rmse\", rmse)\n        run.log_list(\"columns\", train.columns)\n        i += 1"],
        "metadata": {},
        "outputs": [],
        "execution_count": 13
    }, {
        "cell_type": "code",
        "source": ["#%matplotlib inline\n#Load all run metrics from run history into a dictionary object.\nchild_runs = {}\nchild_run_metrics = {}\n\nfor r in root_run.get_children():\n    child_runs[r.id] = r\n    child_run_metrics[r.id] = r.get_metrics()\n\n#Now find the run with the lowest rmse\nbest_run_id = min(child_run_metrics, key = lambda k: child_run_metrics[k]['rmse'])\nbest_run = child_runs[best_run_id]\nprint('Best run is:', best_run_id)\nprint('Metrics:', child_run_metrics[best_run_id])    \n    \nimport matplotlib\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\n\nbest_reg = child_run_metrics[best_run_id]['reg']\nmax_auc = child_run_metrics[best_run_id]['rmse']\n\nreg_auc = np.array([(child_run_metrics[k]['reg'], child_run_metrics[k]['rmse']) for k in child_run_metrics.keys()])\nreg_auc_sorted = reg_auc[reg_auc[:,0].argsort()]\n\nax.plot(reg_auc_sorted[:,0], reg_auc_sorted[:,1], 'r--')\nax.plot(reg_auc_sorted[:,0], reg_auc_sorted[:,1], 'bo')\n\nax.set_xlabel('reg', fontsize = 14)\nax.set_ylabel('rmse', fontsize = 14)\nax.set_title('rmse over reg', fontsize = 16)\n\n# plot arrow\nax.arrow(x = best_reg + 0.45, y = max_auc, dx = -0.4, dy = 0, ls = '-', lw = 0.00001,\n          width = 0.00001, head_width = 0.00002, head_length = 0.02)\n\n# plot \"best run\" text\nax.text(x = best_reg, y = max_auc, s = 'Best Run', fontsize = 14)\n#plt.show()\n\ndisplay(fig)"],
        "metadata": {},
        "outputs": [],
        "execution_count": 14
    }, {
        "cell_type": "code",
        "source": ["# save model\ncvModel.bestModel.write().overwrite().save(model_name)\nprint(\"Model saved\")\n\n# upload the serialized model into run history record\nmdl, ext = model_name.split(\".\")\nmodel_zip = mdl + \".zip\"\nshutil.make_archive(mdl, 'zip', model_dbfs)\nroot_run.upload_file(\"outputs/\" + model_name, model_zip)        \n\n# now delete the serialized model from local folder since it is already uploaded to run history \n# shutil.rmtree(model_dbfs)\nos.remove(model_zip)\n        \n# Declare run completed\nroot_run.complete()\n\n##NOTE: service deployment always gets the model from the current working dir.\nprint(\"copy model from dbfs to local\")\nmodel_local = \"file:\" + os.getcwd() + \"/\" + model_name\ndbutils.fs.cp(model_name, model_local, True)"],
        "metadata": {},
        "outputs": [],
        "execution_count": 15
    }, {
        "cell_type": "code",
        "source": ["pred = cvModel.transform(test)\ndisplay(pred)\n\nre = RegressionEvaluator(metricName=\"rmse\", labelCol=ratingCol,\n                                predictionCol=\"prediction\")\nrmse = re.evaluate(pred)\nprint(\"Root-mean-square error = \" + str(rmse))"],
        "metadata": {},
        "outputs": [],
        "execution_count": 16
    }, {
        "cell_type": "markdown",
        "source": ["# III. Scoring"],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["import json\nwith open('secrets.json') as json_data:\n  writeConfig = json.load(json_data)\n\n  recs = cvModel.bestModel.stages[2].recommendForAllUsers(10)\n  recs.withColumn(\"id\",recs[userColIndex].cast(\"string\")).select(\"id\", \"recommendations.\"+ itemColIndex)\\\n    .write.format(\"com.microsoft.azure.cosmosdb.spark\").mode('overwrite').options(**writeConfig).save()"],
        "metadata": {},
        "outputs": [],
        "execution_count": 18
    }, {
        "cell_type": "markdown",
        "source": ["# IV. Operationalization"],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["#%%writefile score_sparkml.py\n\nscore_sparkml = \"\"\"\n\nimport json\ndef init(local=False):\n    global client, collection\n    try:\n      # Query them in SQL\n      import pydocumentdb.document_client as document_client\n\n      MASTER_KEY = '{key}'\n      HOST = '{endpoint}'\n      DATABASE_ID = \"{database}\"\n      COLLECTION_ID = \"{collection}\"\n      database_link = 'dbs/' + DATABASE_ID\n      collection_link = database_link + '/colls/' + COLLECTION_ID\n      \n      client = document_client.DocumentClient(HOST, {'masterKey': MASTER_KEY})\n      collection = client.ReadCollection(collection_link=collection_link)\n    except Exception as e:\n      collection = e\ndef run(input_json):      \n\n    try:\n      import json\n\n      id = json.loads(json.loads(input_json)[0])['id']\n      query = {'query': 'SELECT * FROM c WHERE c.id = \"' + str(id) +'\"' } #+ str(id)\n\n      options = {}\n\n      result_iterable = client.QueryDocuments(collection['_self'], query, options)\n      result = list(result_iterable);\n  \n    except Exception as e:\n        result = str(e)\n    return json.dumps(str(result)) #json.dumps({{\"result\":result}})\n\"\"\"\n\nimport json\nwith open('secrets.json') as json_data:\n  writeConfig = json.load(json_data)\n  score_sparkml = score_sparkml.replace(\"{key}\",writeConfig['Masterkey']).replace(\"{endpoint}\",writeConfig['Endpoint']).replace(\"{database}\",writeConfig['Database']).replace(\"{collection}\",writeConfig['Collection'])\n\n  exec(score_sparkml)\n\n  with open(\"score_sparkml.py\", \"w\") as file:\n      file.write(score_sparkml)"],
        "metadata": {},
        "outputs": [],
        "execution_count": 20
    }, {
        "cell_type": "code",
        "source": ["%%writefile myenv_sparkml.yml\n\nname: myenv\nchannels:\n  - defaults\ndependencies:\n  - pip:\n    - numpy==1.14.2\n    - scikit-learn==0.19.1\n    - pandas\n    # Required packages for AzureML execution, history, and data preparation.\n    - --extra-index-url https://azuremlsdktestpypi.azureedge.net/sdk-release/Preview/E7501C02541B433786111FE8E140CAA1\n    - azureml-core\n    - pydocumentdb"],
        "metadata": {},
        "outputs": [],
        "execution_count": 21
    }, {
        "cell_type": "code",
        "source": ["from azureml.core.webservice import AciWebservice, Webservice\n#aci = azure container instance\naci_config = AciWebservice.deploy_configuration(\n    cpu_cores = 1, \n    memory_gb = 1, \n    tags = {'name':'Spark ML Databricks sample'}, \n    description = 'This is a great example.')"],
        "metadata": {},
        "outputs": [],
        "execution_count": 22
    }, {
        "cell_type": "code",
        "source": ["from azureml.core.model import Model\nmymodel = Model.register(model_path = model_name, # this points to a local file\n                       model_name = model_name, # this is the name the model is registered as, am using same name for both path and name.                 \n                       description = \"ADB trained model by Dan\",\n                       workspace = ws)\n\nprint(mymodel.name, mymodel.description, mymodel.version)"],
        "metadata": {},
        "outputs": [],
        "execution_count": 23
    }, {
        "cell_type": "code",
        "source": ["models = [mymodel]\nruntime = \"spark-py\"\nconda_file = 'myenv_sparkml.yml'\ndriver_file = \"score_sparkml.py\"\n\n# image creation\nfrom azureml.core.image import ContainerImage\nmyimage_config = ContainerImage.image_configuration(execution_script = driver_file, \n                                    runtime = runtime, \n                                    conda_file = conda_file)\n"],
        "metadata": {},
        "outputs": [],
        "execution_count": 24
    }, {
        "cell_type": "code",
        "source": ["# List images by ws\n\nfrom azureml.core.image import ContainerImage\nfor i in ContainerImage.list(workspace = ws):\n    print('{}(v.{} [{}]) stored at {} with build log {}'.format(i.name, i.version, i.creation_state, i.image_location, i.image_build_log_uri))"],
        "metadata": {},
        "outputs": [],
        "execution_count": 25
    }, {
        "cell_type": "markdown",
        "source": ["# a. ACI Deployment"],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["try:\n  myservice = AciWebservice(ws, name=service_name)\nexcept:\n  myservice = Webservice.deploy_from_model(\n    workspace=ws, \n    name=service_name,\n    deployment_config = aci_config,\n    models = models,\n    image_config = myimage_config\n      )\n\n  myservice.wait_for_deployment(show_output=True)\n\nprint(yaml.dump(myservice.__dict__, default_flow_style=False))"],
        "metadata": {},
        "outputs": [],
        "execution_count": 27
    }, {
        "cell_type": "code",
        "source": ["json2 = '[\"{\\\\\"id\\\\\":\\\\\"5616\\\\\"}\"]'.encode()\nres1_service = myservice.run(input_data = json2)\nprint(res1_service)"],
        "metadata": {},
        "outputs": [],
        "execution_count": 28
    }, {
        "cell_type": "markdown",
        "source": ["# b. AKS Deployment"],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["from azureml.core.image import Image\nmyimage = Image(workspace=ws, id=\"movies-als:1\")"],
        "metadata": {},
        "outputs": [],
        "execution_count": 30
    }, {
        "cell_type": "code",
        "source": ["#create AKS compute\n#it may take 20-25 minutes to create a new cluster\n\nfrom azureml.core.compute import AksCompute, ComputeTarget\n\n# Use the default configuration (can also provide parameters to customize)\nprov_config = AksCompute.provisioning_configuration()\n\n# Create the cluster\naks_target = ComputeTarget.create(workspace = ws, \n                                  name = aks_name, \n                                  provisioning_configuration = prov_config)\n\naks_target.wait_for_completion(show_output = True)\n\nprint(aks_target.provisioning_state)\nprint(aks_target.provisioning_errors)"],
        "metadata": {},
        "outputs": [],
        "execution_count": 31
    }, {
        "cell_type": "code",
        "source": ["from azureml.core.webservice import Webservice, AksWebservice\nfrom azureml.core.image import ContainerImage\n\n#Set the web service configuration (using default here with app insights)\naks_config = AksWebservice.deploy_configuration(enable_app_insights=True)\n\n# Webservice creation using single command, there is a variant to use image directly as well.\naks_service = Webservice.deploy_from_image(\n  workspace=ws, \n  name=service_name,\n  deployment_config = aks_config,\n  image = myimage,\n  deployment_target = aks_target\n    )\n\naks_service.wait_for_deployment(show_output=True)"],
        "metadata": {},
        "outputs": [],
        "execution_count": 32
    }, {
        "cell_type": "code",
        "source": ["scoring_url = aks_service.scoring_uri\nservice_key = aks_service.get_keys()[0]"],
        "metadata": {},
        "outputs": [],
        "execution_count": 33
    }, {
        "cell_type": "code",
        "source": ["json2 = '[\"{\\\\\"id\\\\\":\\\\\"5616\\\\\"}\"]'.encode()"],
        "metadata": {},
        "outputs": [],
        "execution_count": 34
    }, {
        "cell_type": "code",
        "source": ["import urllib\nimport time\nimport json\n\nreq = urllib.request.Request(scoring_url,data=json2)\nreq.add_header(\"Authorization\",\"Bearer {}\".format(service_key))\nreq.add_header(\"Content-Type\",\"application/json\")\n\ntic = time.time()\nwith urllib.request.urlopen(req) as result:\n    res = result.readlines()\n    print(res)\n    \ntoc = time.time()\nt2 = toc - tic\nprint(\"Full run took %.2f seconds\" % (toc - tic))"],
        "metadata": {},
        "outputs": [],
        "execution_count": 35
    }, {
        "cell_type": "code",
        "source": [""],
        "metadata": {},
        "outputs": [],
        "execution_count": 36
    }],
    "metadata": {
        "name": "ALS_Movie_Example",
        "notebookId": 1090923442739622
    },
    "nbformat": 4,
    "nbformat_minor": 0
}

{"cells":[{"cell_type":"markdown","source":["### News Recommendation ALS Example Databricks Notebook\n##### by Daniel Ciborowski, dciborow@microsoft.com\n\n##### Copyright (c) Microsoft Corporation. All rights reserved.\n\n##### Licensed under the MIT License.\n\n##### Setup\n1. Create new Cluster, DB 4.1, Spark 2.3.0, Python3\n1. (Optional for Ranking Metrics) From Maven add to cluster the following jar: Azure:mmlspark:0.15\n\nIn a news recommendation scenario, items have an active lifespan when they should be recommended. After this time has expired old stories are not recommended, and new news stories replace the expired ones. When recommending new stories, only active stories should be recommended. This example shows how to train a model using historical data, and make recommendations for the latest news stories.\n\nNew Recommendation Dataset can be found here. http://reclab.idi.ntnu.no/dataset/\n\n##### Citation\nGulla, J. A., Zhang, L., Liu, P., Özgöbek, Ö., & Su, X. (2017, August). The Adressa dataset for news recommendation. In Proceedings of the International Conference on Web Intelligence (pp. 1042-1048). ACM."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nimport random\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import col, collect_list"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Create Sample Data\nraw = [\n  {'userId': 1, 'itemId': 1, 'rating':  random.randint(0, 10), 'timestamp': 1462277923},\n  {'userId': 2, 'itemId': 1, 'rating':  random.randint(0, 10), 'timestamp': 1463455636},\n  {'userId': 3, 'itemId': 1, 'rating':  random.randint(0, 10), 'timestamp': 1464277923},\n  {'userId': 4, 'itemId': 1, 'rating':  random.randint(0, 10), 'timestamp': 1465277923},\n  {'userId': 5, 'itemId': 1, 'rating':  random.randint(0, 10), 'timestamp': 1466277923},\n  {'userId': 1, 'itemId': 2, 'rating':  random.randint(0, 10), 'timestamp': 1467277923},\n  {'userId': 2, 'itemId': 2, 'rating':  random.randint(0, 10), 'timestamp': 1468277923},\n  {'userId': 3, 'itemId': 2, 'rating':  random.randint(0, 10), 'timestamp': 1469277923},\n  {'userId': 4, 'itemId': 2, 'rating':  random.randint(0, 10), 'timestamp': 1471277923},\n  {'userId': 5, 'itemId': 2, 'rating':  random.randint(0, 10), 'timestamp': 1472277923},\n  {'userId': 1, 'itemId': 3, 'rating':  random.randint(0, 10), 'timestamp': 1473277923},\n  {'userId': 2, 'itemId': 3, 'rating':  random.randint(0, 10), 'timestamp': 1474277923},\n  {'userId': 3, 'itemId': 3, 'rating':  random.randint(0, 10), 'timestamp': 1475277923},\n  {'userId': 4, 'itemId': 3, 'rating':  random.randint(0, 10), 'timestamp': 1476277923},\n  {'userId': 5, 'itemId': 3, 'rating':  random.randint(0, 10), 'timestamp': 1477277923},\n  {'userId': 1, 'itemId': 4, 'rating':  random.randint(0, 10), 'timestamp': 1478277923},\n  {'userId': 2, 'itemId': 4, 'rating':  random.randint(0, 10), 'timestamp': 1479277923},\n  {'userId': 3, 'itemId': 4, 'rating':  random.randint(0, 10), 'timestamp': 1481277923},\n  {'userId': 4, 'itemId': 4, 'rating':  random.randint(0, 10), 'timestamp': 1482277923},\n  {'userId': 5, 'itemId': 4, 'rating':  random.randint(0, 10), 'timestamp': 1483277923},  \n  {'userId': 1, 'itemId': 5, 'rating':  random.randint(0, 10), 'timestamp': 1484277923},\n  {'userId': 2, 'itemId': 5, 'rating':  random.randint(0, 10), 'timestamp': 1485277923},\n  {'userId': 3, 'itemId': 5, 'rating':  random.randint(0, 10), 'timestamp': 1486277923},\n  {'userId': 4, 'itemId': 5, 'rating':  random.randint(0, 10), 'timestamp': 1487277923},\n  {'userId': 5, 'itemId': 5, 'rating':  random.randint(0, 10), 'timestamp': 1492455636},   \n]\n\nday1 = pd.DataFrame(raw)\nday2=pd.DataFrame(raw)\nday2['itemId'] = day2['itemId']+10\nday2['timestamp'] = day2['timestamp']+100000000\nday3=pd.DataFrame(raw)\nday3['itemId'] = day3['itemId']+20\nday3['timestamp'] = day3['timestamp']+200000000\nday4=pd.DataFrame(raw)\nday4['itemId'] = day4['itemId']+30\nday4['timestamp'] = day4['timestamp']+300000000\n\ndata = day1 \\\n  .append(day2) \\\n  .append(day3) \\\n  .append(day4) \\\n  .sample(frac=0.75, replace=False)\n\nspark = SparkSession.builder.getOrCreate()\nratings = spark.createDataFrame(data)\ndisplay(ratings.select('userId','itemId','rating','timestamp').orderBy('userId','itemId'))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["display(ratings.select('userId','itemId','rating','timestamp').orderBy('userId','itemId'))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Build the recommendation model using ALS on the rating data\n# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nalgo = ALS(userCol=\"userId\", itemCol=\"itemId\", implicitPrefs=True, coldStartStrategy=\"drop\")\nmodel = algo.fit(ratings)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Evaluate the model by computing the RMSE on the rating data\npredictions = model.transform(ratings)\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n                                predictionCol=\"prediction\")\nrmse = evaluator.evaluate(predictions)\nprint(\"Root-mean-square error = \" + str(rmse))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Evaluate the model by computing ranking metrics on the rating data\nfrom mmlspark.RankingAdapter import RankingAdapter\nfrom mmlspark.RankingEvaluator import RankingEvaluator\n\noutput = RankingAdapter(mode='allUsers', k=5, recommender=algo) \\\n  .fit(ratings) \\\n  .transform(ratings)\n\nmetrics = ['ndcgAt','map','recallAtK','mrr','fcp']\nmetrics_dict = {}\nfor metric in metrics:\n    metrics_dict[metric] = RankingEvaluator(k=3, metricName=metric).evaluate(output)\n\nmetrics_dict    "],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Recommend Subset Wrapper\ndef recommendSubset(self, df, timestamp):\n  def Func(lines):\n    out = []\n    for i in range(len(lines[1])):\n      out += [(lines[1][i],lines[2][i])]\n    return lines[0], out\n\n  tup = StructType([\n    StructField('itemId', IntegerType(), True),\n    StructField('rating', FloatType(), True)\n  ])\n  array_type = ArrayType(tup, True)\n  active_items = df.filter(col(\"timestamp\") > timestamp).select(\"itemId\").distinct()\n  users = df.select(\"userId\").distinct()\n\n  users_active_items = users.crossJoin(active_items)\n  scored = self.transform(users_active_items)\n\n  recs = scored \\\n    .groupBy(col('userId')) \\\n    .agg(collect_list(col(\"itemId\")),collect_list(col(\"prediction\"))) \\\n    .rdd \\\n    .map(Func) \\\n    .toDF() \\\n    .withColumnRenamed(\"_1\",\"userId\") \\\n    .withColumnRenamed(\"_2\",\"recommendations\") \\\n    .select(col(\"userId\"),col(\"recommendations\").cast(array_type))\n\n  return recs\n\nimport pyspark\npyspark.ml.recommendation.ALSModel.recommendSubset = recommendSubset"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Recommend most recent items for all users\nrecs = model.recommendSubset(ratings, 1662277923)\n\ndisplay(recs.orderBy('userId'))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["In order to turn new stories from cold items, to warm items, 1% of the recommendations servered should include a random new (cold) story. This population should also be used to provide a baseline to measure the online model performance."],"metadata":{}},{"cell_type":"markdown","source":["# Repeat with a larger dataset.\n\n1 Week of data collection - 923 articles (in Norwegian), 15,514 users, average article length is 518.6 words"],"metadata":{}},{"cell_type":"code","source":["spark = SparkSession.builder.getOrCreate()\n\ndata = spark.read.json(\"wasb://sampledata@dcibviennadata.blob.core.windows.net/one_week.json\") \\\n  .cache()\n\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline, PipelineModel\n\ndf = data \\\n  .filter(col(\"sessionStart\") != 'true') \\\n  .filter(col(\"sessionStop\") != 'true') \\\n  .filter(col(\"url\") != \"http://adressa.no\") \\\n  .filter(col(\"activeTime\") > 10) \\\n  .select(\"userId\",\"url\", \"activeTime\", \"time\") \\\n  .cache()\n\n\nindexerContacts = StringIndexer(inputCol='userId', outputCol='userIdIndex', handleInvalid='keep').fit(df)\nindexerRules = StringIndexer(inputCol='url', outputCol='itemIdIndex', handleInvalid='keep').fit(df)\n\nratings = indexerRules.transform(indexerContacts.transform(df)) \\\n  .select(\"userIdIndex\",\"itemIdIndex\",\"activeTime\",\"time\") \\\n  .withColumnRenamed('userIdIndex',\"userId\") \\\n  .withColumnRenamed('itemIdIndex',\"itemId\") \\\n  .withColumnRenamed('activeTime',\"rating\") \\\n  .withColumnRenamed('time',\"timestamp\") \\\n  .cache()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["display(ratings.select('userId','itemId','rating','timestamp').orderBy('userId','itemId'))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["display(ratings.select('userId','itemId','rating','timestamp').orderBy('userId','itemId'))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Build the recommendation model using ALS on the rating data\n# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nalgo = ALS(userCol=\"userId\", itemCol=\"itemId\", implicitPrefs=True, coldStartStrategy=\"drop\")\nmodel = algo.fit(ratings)\n\n# Evaluate the model by computing the RMSE on the rating data\npredictions = model.transform(ratings)\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n                                predictionCol=\"prediction\")\nrmse = evaluator.evaluate(predictions)\nprint(\"Root-mean-square error = \" + str(rmse))\n\n# Evaluate the model by computing ranking metrics on the rating data\nfrom mmlspark.RankingAdapter import RankingAdapter\nfrom mmlspark.RankingEvaluator import RankingEvaluator\n\noutput = RankingAdapter(mode='allUsers', k=5, recommender=algo) \\\n  .fit(ratings) \\\n  .transform(ratings)\n\nmetrics = ['ndcgAt','map','recallAtK','mrr','fcp']\nmetrics_dict = {}\nfor metric in metrics:\n    metrics_dict[metric] = RankingEvaluator(k=3, metricName=metric).evaluate(output)\n\nprint(metrics_dict)\n\n# Recommend most recent items for all users\nrecs = model.recommendSubset(ratings, 1483747200) \\\n  .cache()\n\nrecs.take(5)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":16}],"metadata":{"name":"News Recommendations - ALS","notebookId":3319220000219654},"nbformat":4,"nbformat_minor":0}

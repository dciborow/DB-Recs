{
    "cells": [{
        "cell_type": "markdown",
        "source": ["### News Recommendation ALS w/ AML Example Databricks Notebook\n##### by Daniel Ciborowski, dciborow@microsoft.com\n\n##### Copyright (c) Microsoft Corporation. All rights reserved.\n\n##### Licensed under the MIT License.\n\n##### Setup\n1. Create new Cluster, DB 4.1, Spark 2.3.0, Python3\n1. (Optional for Ranking Metrics) From Maven add to cluster the following jar: Azure:mmlspark:0.15\n1. Cosmos DB Uber Jar - https://repo1.maven.org/maven2/com/microsoft/azure/azure-cosmosdb-spark_2.3.0_2.11/1.2.7/azure-cosmosdb-spark_2.3.0_2.11-1.2.7-uber.jar\n\n##### This notebook is broken down into four sections.\n1. Experimentation\n1. Training & Scoring\n1. Serving\n\n##### The following Azure services will be deployed into a new or existing resource group.\n1. [ML Service](https://docs.databricks.com/user-guide/libraries.html)\n1. [Cosmos DB](https://azure.microsoft.com/en-us/services/cosmos-db/)\n1. [Container Registery](https://docs.microsoft.com/en-us/azure/container-registry/)\n1. [Container Instances](https://docs.microsoft.com/en-us/azure/container-instances/)\n1. [Application Insights](https://azure.microsoft.com/en-us/services/monitor/)\n1. Storage Account\n1. Key Vault\n\nIn a news recommendation scenario, items have an active lifespan when they should be recommended. After this time has expired old stories are not recommended, and new news stories replace the expired ones. When recommending new stories, only active stories should be recommended. This example shows how to train a model using historical data, and make recommendations for the latest news stories.\n\n![Design](https://raw.githubusercontent.com/dciborow/DB-Recs/master/NewsRecs/reco_news_design.JPG)\n\n\nIn order to turn new stories from cold items, to warm items, 1% of the recommendations servered should include a random new (cold) story. This population should also be used to provide a baseline to measure the online model performance.\n\nNew Recommendation Dataset can be found here. http://reclab.idi.ntnu.no/dataset/\n\n##### Citation\nGulla, J. A., Zhang, L., Liu, P., Özgöbek, Ö., & Su, X. (2017, August). The Adressa dataset for news recommendation. In Proceedings of the International Conference on Web Intelligence (pp. 1042-1048). ACM."],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["import pandas as pd\nimport random\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import col, collect_list"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]
            }
        }],
        "execution_count": 2
    }, {
        "cell_type": "code",
        "source": ["from azure.common.client_factory import get_client_from_cli_profile\n\nimport azureml.core\nfrom azureml.core import Workspace\nfrom azureml.core.run import Run\nfrom azureml.core.experiment import Experiment\n\n\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.sql import Row\n\nimport numpy as np\nimport os\nimport pandas as pd\nimport pprint\nimport shutil\nimport time, timeit\nimport urllib\nimport yaml\n\n# Check core SDK version number - based on build number of preview/master.\nprint(\"SDK version:\", azureml.core.VERSION)\n\nprefix = \"dcib_igor_\"\nsubscription_id = ''\ndata = 'news'\n\nworkspace_region = \"westus2\"\nresource_group = prefix + \"_\" + data\nworkspace_name = prefix + \"_\"+data+\"_aml\"\nexperiment_name = data + \"_als_Experiment\"\naks_name = \"dcibigoraks\"\nservice_name = \"dcibigoraksals\"\n\n# import the Workspace class and check the azureml SDK version\n# exist_ok checks if workspace exists or not.\nws = Workspace.create(name = workspace_name,\n                      subscription_id = subscription_id,\n                      resource_group = resource_group, \n                      location = workspace_region,\n                      exist_ok=True)\n\n# persist the subscription id, resource group name, and workspace name in aml_config/config.json.\nws.write_config()\n\n# start a training run by defining an experiment\nmyexperiment = Experiment(ws, experiment_name)\nroot_run = myexperiment.start_logging()\n"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">SDK version: 0.1.80\nWrote the config file config.json to: /databricks/driver/aml_config/config.json\n</div>"]
            }
        }],
        "execution_count": 3
    }, {
        "cell_type": "markdown",
        "source": ["AzureML is a way to organize your Machine Learning development process. It can be used directly from the Azure portal, or programmatically from a notebook like in this example.\n![Design](https://raw.githubusercontent.com/dciborow/DB-Recs/master/NewsRecs/workspace.JPG)"],
        "metadata": {}
    }, {
        "cell_type": "markdown",
        "source": ["# I. Experimentation"],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["spark = SparkSession.builder.getOrCreate()\n\ndata = spark.read.json(\"wasb://sampledata@dcibviennadata.blob.core.windows.net/one_week.json\") \\\n  .cache()\n\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline, PipelineModel\n\ndf = data \\\n  .filter(col(\"sessionStart\") != 'true') \\\n  .filter(col(\"sessionStop\") != 'true') \\\n  .filter(col(\"url\") != \"http://adressa.no\") \\\n  .filter(col(\"activeTime\") > 10) \\\n  .select(\"userId\",\"url\", \"activeTime\", \"time\") \\\n  .cache()\n\nindexerContacts = StringIndexer(inputCol='userId', outputCol='userIdIndex', handleInvalid='keep').fit(df)\nindexerRules = StringIndexer(inputCol='url', outputCol='itemIdIndex', handleInvalid='keep').fit(df)\n\nratings = indexerRules.transform(indexerContacts.transform(df)) \\\n  .select(\"userIdIndex\",\"itemIdIndex\",\"activeTime\",\"time\") \\\n  .withColumnRenamed('userIdIndex',\"userId\") \\\n  .withColumnRenamed('itemIdIndex',\"itemId\") \\\n  .withColumnRenamed('activeTime',\"rating\") \\\n  .withColumnRenamed('time',\"timestamp\") \\\n  .cache()\n\ndisplay(ratings.select('userId','itemId','rating','timestamp').orderBy('userId','itemId'))"],
        "metadata": {},
        "execution_count": 6
    }, {
        "cell_type": "code",
        "source": ["display(ratings.select('userId','itemId','rating','timestamp').orderBy('userId','itemId'))"],
        "metadata": {},
        "execution_count": 7
    }, {
        "cell_type": "code",
        "source": ["# Build the recommendation model using ALS on the rating data\n# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nalgo = ALS(userCol=\"userId\", itemCol=\"itemId\", implicitPrefs=True, coldStartStrategy=\"drop\")\nmodel = algo.fit(ratings)"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]
            }
        }],
        "execution_count": 8
    }, {
        "cell_type": "code",
        "source": ["# Evaluate the model by computing ranking metrics on the rating data\nfrom mmlspark.RankingAdapter import RankingAdapter\nfrom mmlspark.RankingEvaluator import RankingEvaluator\n\noutput = RankingAdapter(mode='allUsers', k=5, recommender=algo) \\\n  .fit(ratings) \\\n  .transform(ratings)\n\nmetrics = ['ndcgAt','map','recallAtK','mrr','fcp']\nmetrics_dict = {}\nfor metric in metrics:\n    metrics_dict[metric] = RankingEvaluator(k=3, metricName=metric).evaluate(output)\n\nfor k in metrics_dict:\n  root_run.log(k, metrics_dict[k])    \n  \ndisplay(spark.createDataFrame(pd.DataFrame(list(root_run.get_metrics().items()), columns=['metric','value'])))   "],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>metric</th><th>value</th></tr></thead><tbody><tr><td>map</td><td>0.3410084486390418</td></tr><tr><td>ndcgAt</td><td>0.3831677112362854</td></tr><tr><td>rmse</td><td>94.22848622877933</td></tr><tr><td>recallAtK</td><td>0.18540269574317148</td></tr><tr><td>mrr</td><td>0.4864189232443509</td></tr><tr><td>fcp</td><td>0.21253212250907655</td></tr></tbody></table></div>"]
            }
        }],
        "execution_count": 9
    }, {
        "cell_type": "code",
        "source": ["%%writefile recommend.py\n\nimport pyspark\nfrom pyspark.ml.recommendation import ALS\n\n# Recommend Subset Wrapper\ndef recommendSubset(self, df, timestamp):\n  def Func(lines):\n    out = []\n    for i in range(len(lines[1])):\n      out += [(lines[1][i],lines[2][i])]\n    return lines[0], out\n\n  tup = StructType([\n    StructField('itemId', IntegerType(), True),\n    StructField('rating', FloatType(), True)\n  ])\n  array_type = ArrayType(tup, True)\n  active_items = df.filter(col(\"timestamp\") > timestamp).select(\"itemId\").distinct()\n  users = df.select(\"userId\").distinct()\n\n  users_active_items = users.crossJoin(active_items)\n  scored = self.transform(users_active_items)\n\n  recs = scored \\\n    .groupBy(col('userId')) \\\n    .agg(collect_list(col(\"itemId\")),collect_list(col(\"prediction\"))) \\\n    .rdd \\\n    .map(Func) \\\n    .toDF() \\\n    .withColumnRenamed(\"_1\",\"userId\") \\\n    .withColumnRenamed(\"_2\",\"recommendations\") \\\n    .select(col(\"userId\"),col(\"recommendations\").cast(array_type))\n\n  return recs\n\nimport pyspark\npyspark.ml.recommendation.ALSModel.recommendSubset = recommendSubset\n\n#Implement this function\ndef recommend(historic, timestamp):   \n  algo = ALS(userCol=\"userId\", itemCol=\"itemId\", implicitPrefs=True, coldStartStrategy=\"drop\")\n  model = algo.fit(historic)  \n  recs = model.recommendSubset(historic, timestamp)\n  return recs"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Overwriting recommend.py\n</div>"]
            }
        }],
        "execution_count": 10
    }, {
        "cell_type": "code",
        "source": ["with open('recommend.py', 'r') as myfile:\n    data=myfile.read()\n\nexec(data)\n\nrecs = recommend(ratings, 1483747200) \\\n  .cache()\n# display(recs.orderBy('userId'))\n\nrecs.take(1)"],
        "metadata": {},
        "outputs": [],
        "execution_count": 11
    }, {
        "cell_type": "code",
        "source": ["root_run.upload_file(\"outputs/recommend.py\",'recommend.py')\nroot_run.complete()\n\nprint(\"Run Url: \" + root_run.get_portal_url())"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]
            }
        }],
        "execution_count": 12
    }, {
        "cell_type": "markdown",
        "source": ["AzureML has recorded this experiment and provides a configurable dashboard for run history.\n![Design](https://raw.githubusercontent.com/dciborow/DB-Recs/master/NewsRecs/experiment.JPG)"],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["# Register as model\nfrom azureml.core.model import Model\nmymodel = Model.register(model_path = 'recommend.py', # this points to a local file\n                       model_name = 'als', # this is the name the model is registered as, am using same name for both path and name.                 \n                       description = \"ADB trained model by Dan\",\n                       workspace = ws)\n\nprint(mymodel.name, mymodel.description, mymodel.version)\nprint(\"URL: \" + mymodel.url)"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Registering model als\nals ADB trained model by Dan 6\nURL: aml://asset/6b3bfc70d79640dc9bd0ab343663b55c\n</div>"]
            }
        }],
        "execution_count": 14
    }, {
        "cell_type": "markdown",
        "source": ["The model becomes a trackable asset in the Machine Learning workspace.\n\n![Model](https://raw.githubusercontent.com/dciborow/DB-Recs/master/NewsRecs/model.JPG)"],
        "metadata": {}
    }, {
        "cell_type": "markdown",
        "source": ["# II. Training and Scoring"],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["from azureml.core.model import Model\n\nmymodel = Model.list(ws)[0]\nmymodel.download('./o16n/',exists_ok=True)\nprint(mymodel.name, mymodel.description, mymodel.version)\n\nwith open('./o16n/recommend.py', 'r') as myfile:\n    data=myfile.read()\n\nexec(data)\n\nrecs = recommend(ratings, 1483747200) \\\n  .cache()\n\nrecs.take(1)"],
        "metadata": {},
        "outputs": [],
        "execution_count": 17
    }, {
        "cell_type": "code",
        "source": ["account_name = \"movies-ds-sql\"\nendpoint = \"https://\" + account_name + \".documents.azure.com:443/\"\nmaster_key = \"\"\n\nwriteConfig = {\n  \"Endpoint\": endpoint,\n  \"Masterkey\": master_key,\n  \"Database\": 'recommendations',\n  \"Collection\": 'news',\n  \"Upsert\": \"true\"\n}\n\n# recs \\\n#   .withColumn(\"id\",recs['userid'].cast(\"string\")) \\\n#   .select(\"id\", \"recommendations.itemid\")\\\n#   .write \\\n#   .format(\"com.microsoft.azure.cosmosdb.spark\") \\\n#   .mode('overwrite') \\\n#   .options(**writeConfig) \\\n#   .save()"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]
            }
        }],
        "execution_count": 18
    }, {
        "cell_type": "markdown",
        "source": ["Store recommendations in Cosmos DB for low latency serving.\n\n![Cosmos DB](https://raw.githubusercontent.com/dciborow/DB-Recs/master/NewsRecs/cosmosdb.JPG)"],
        "metadata": {}
    }, {
        "cell_type": "markdown",
        "source": ["# III. Serving"],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["from azureml.core.model import Model\n\nmymodel = Model.list(ws)[0]\nprint(mymodel.name, mymodel.description, mymodel.version)"],
        "metadata": {},
        "outputs": [],
        "execution_count": 21
    }, {
        "cell_type": "code",
        "source": ["%%writefile score_sparkml.py\n\nimport json\ndef init(local=False):\n    global client, collection\n    try:\n      # Query them in SQL\n      import pydocumentdb.document_client as document_client\n\n      MASTER_KEY = '{key}'\n      HOST = '{endpoint}'\n      DATABASE_ID = \"{database}\"\n      COLLECTION_ID = \"{collection}\"\n      database_link = 'dbs/' + DATABASE_ID\n      collection_link = database_link + '/colls/' + COLLECTION_ID\n      \n      client = document_client.DocumentClient(HOST, {'masterKey': MASTER_KEY})\n      collection = client.ReadCollection(collection_link=collection_link)\n    except Exception as e:\n      collection = e\ndef run(input_json):      \n\n    try:\n      import json\n\n      id = json.loads(json.loads(input_json)[0])['id']\n      query = {'query': 'SELECT * FROM c WHERE c.id = \"' + str(id) +'\"' } #+ str(id)\n\n      options = {}\n\n      result_iterable = client.QueryDocuments(collection['_self'], query, options)\n      result = list(result_iterable);\n  \n    except Exception as e:\n        result = str(e)\n    return json.dumps(str(result)) #json.dumps({{\"result\":result}})"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Overwriting score_sparkml.py\n</div>"]
            }
        }],
        "execution_count": 22
    }, {
        "cell_type": "code",
        "source": ["# Test Web Service Code\nwith open('score_sparkml.py', 'r') as myfile:\n    score_sparkml=myfile.read()\n    \nimport json\nscore_sparkml = score_sparkml.replace(\"{key}\",writeConfig['Masterkey']).replace(\"{endpoint}\",writeConfig['Endpoint']).replace(\"{database}\",writeConfig['Database']).replace(\"{collection}\",writeConfig['Collection'])\n\nexec(score_sparkml)"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]
            }
        }],
        "execution_count": 23
    }, {
        "cell_type": "code",
        "source": ["%%writefile myenv_sparkml.yml\n\nname: myenv\nchannels:\n  - defaults\ndependencies:\n  - pip:\n    - numpy==1.14.2\n    - scikit-learn==0.19.1\n    - pandas\n    # Required packages for AzureML execution, history, and data preparation.\n    - --extra-index-url https://azuremlsdktestpypi.azureedge.net/sdk-release/Preview/E7501C02541B433786111FE8E140CAA1\n    - azureml-core\n    - pydocumentdb"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Overwriting myenv_sparkml.yml\n</div>"]
            }
        }],
        "execution_count": 24
    }, {
        "cell_type": "code",
        "source": ["# Create Image for Web Service\nmodels = [mymodel]\nruntime = \"spark-py\"\nconda_file = 'myenv_sparkml.yml'\ndriver_file = \"score_sparkml.py\"\n\n# image creation\nfrom azureml.core.image import ContainerImage\nmyimage_config = ContainerImage.image_configuration(execution_script = driver_file, \n                                    runtime = runtime, \n                                    conda_file = conda_file)\n\nimage = ContainerImage.create(name = \"news-als\",\n                                # this is the model object\n                                models = [mymodel],\n                                image_config = myimage_config,\n                                workspace = ws)\n\n# Wait for the create process to complete\nimage.wait_for_creation(show_output = True)"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Creating image\nRunning...............\nSucceededImage creation operation finished for image news-als:7, operation &quot;Succeeded&quot;\n</div>"]
            }
        }],
        "execution_count": 25
    }, {
        "cell_type": "markdown",
        "source": ["AzureML tracks images used for web services.\n\n![Image](https://raw.githubusercontent.com/dciborow/DB-Recs/master/NewsRecs/image.JPG)"],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["#create AKS compute\n#it may take 20-25 minutes to create a new cluster\n\nfrom azureml.core.compute import AksCompute, ComputeTarget\n\n# Use the default configuration (can also provide parameters to customize)\nprov_config = AksCompute.provisioning_configuration()\n\n# Create the cluster\naks_target = ComputeTarget.create(workspace = ws, \n                                  name = aks_name, \n                                  provisioning_configuration = prov_config)\n\naks_target.wait_for_completion(show_output = True)\n\nprint(aks_target.provisioning_state)\nprint(aks_target.provisioning_errors)"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">SucceededProvisioning operation finished, operation &quot;Succeeded&quot;\nSucceeded\nNone\n</div>"]
            }
        }],
        "execution_count": 27
    }, {
        "cell_type": "markdown",
        "source": ["Azure Kubernetes service is used to host the endpoint. \n\n![Image](https://raw.githubusercontent.com/dciborow/DB-Recs/master/NewsRecs/aks.JPG)"],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["# Deploy image to AKS\n\nfrom azureml.core.webservice import Webservice, AksWebservice\nfrom azureml.core.image import ContainerImage\n\n#Set the web service configuration (using default here with app insights)\naks_config = AksWebservice.deploy_configuration(enable_app_insights=True)\n\n# Webservice creation using single command, there is a variant to use image directly as well.\ntry:\n  aks_service = Webservice.deploy_from_image(\n    workspace=ws, \n    name=service_name,\n    deployment_config = aks_config,\n    image = image,\n    deployment_target = aks_target\n      )\n  aks_service.wait_for_deployment(show_output=True)\nexcept Exception:\n    aks_service = Webservice.list(ws)[0]\n"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]
            }
        }],
        "execution_count": 29
    }, {
        "cell_type": "markdown",
        "source": ["AzureML provides more details about this deployment\n\n![Deployment](https://raw.githubusercontent.com/dciborow/DB-Recs/master/NewsRecs/deployment.JPG)\n\nIncluding ML linage back to model used to produce recommendations served by this service.\n\n![Deployment](https://raw.githubusercontent.com/dciborow/DB-Recs/master/NewsRecs/deployment_models.JPG)"],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": ["import urllib\nimport time\nimport json\n\nscoring_url = aks_service.scoring_uri\nservice_key = aks_service.get_keys()[0]\n\ninput_data = '[\"{\\\\\"id\\\\\":\\\\\"1\\\\\"}\"]'.encode()\n\nreq = urllib.request.Request(scoring_url,data=input_data)\nreq.add_header(\"Authorization\",\"Bearer {}\".format(service_key))\nreq.add_header(\"Content-Type\",\"application/json\")\n\ntic = time.time()\nwith urllib.request.urlopen(req) as result:\n    res = result.readlines()\n    print(res)\n    \ntoc = time.time()\nt2 = toc - tic\nprint(\"Full run took %.2f seconds\" % (toc - tic))"],
        "metadata": {},
        "outputs": [{
            "metadata": {},
            "output_type": "display_data",
            "data": {
                "text/html": ["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[b&apos;&quot;\\\\&quot;[{\\&apos;itemid\\&apos;: [31, 34, 35, 32, 33], \\&apos;id\\&apos;: \\&apos;1\\&apos;, \\&apos;_rid\\&apos;: \\&apos;lWAHAOmH9LcBAAAAAAAAAA==\\&apos;, \\&apos;_etag\\&apos;: \\&apos;\\\\\\\\\\\\&quot;0200bc7b-0000-0000-0000-5c032e8a0000\\\\\\\\\\\\&quot;\\&apos;, \\&apos;_ts\\&apos;: 1543712394, \\&apos;_self\\&apos;: \\&apos;dbs/lWAHAA==/colls/lWAHAOmH9Lc=/docs/lWAHAOmH9LcBAAAAAAAAAA==/\\&apos;, \\&apos;_attachments\\&apos;: \\&apos;attachments/\\&apos;}]\\\\&quot;&quot;&apos;]\nFull run took 0.06 seconds\n</div>"]
            }
        }],
        "execution_count": 31
    }, {
        "cell_type": "markdown",
        "source": ["Application Insights is automatically set up, and tracks response time, requests, and more!\n\n![Deployment](https://raw.githubusercontent.com/dciborow/DB-Recs/master/NewsRecs/appinsights.JPG)"],
        "metadata": {}
    }, {
        "cell_type": "code",
        "source": [""],
        "metadata": {},
        "outputs": [],
        "execution_count": 33
    }],
    "metadata": {
        "name": "News Recommendations - ALS - AML",
        "notebookId": 2996767278517779
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
